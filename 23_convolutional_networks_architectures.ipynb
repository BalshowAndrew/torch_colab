{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e181825e",
   "metadata": {},
   "source": [
    "# Convolutional Networks Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422c7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as tv\n",
    "from torchsummary import summary\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81bf257e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b9a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device):\n",
    "    acc_sum, n = 0, 0\n",
    "    net.eval()\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        acc_sum +=(net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum.item() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e7e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, optimizer, num_epochs, device):\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Step {i}, time since epoch: {time.time() - start:.3f}. \"\n",
    "                      f\"Train acc: {train_acc_sum / n:.3f}. Train Loss: {train_l_sum / n:.3f}\")\n",
    "        test_acc = evaluate_accuracy(test_iter, net.to(device), device)\n",
    "        print(f\"epoch {epoch + 1}, loss {train_l_sum / n:.4f}, train acc {train_acc_sum / n:.3f}\" \\\n",
    "              f\", test acc {test_acc:.3f}, time {time.time() - start:.1f} sec\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1268d13",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f70cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "transforms = tv.transforms.Compose([\n",
    "    tv.transforms.Resize((224, 224)),\n",
    "    tv.transforms.ToTensor()\n",
    "])\n",
    "train_dataset = tv.datasets.MNIST('./datas', train=True, transform=transforms, download=True)\n",
    "test_dataset = tv.datasets.MNIST('./datas', train=False, transform=transforms, download=True)\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a842a8",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87df0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(6400, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8081855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 54, 54]          11,712\n",
      "              ReLU-2           [-1, 96, 54, 54]               0\n",
      "         MaxPool2d-3           [-1, 96, 26, 26]               0\n",
      "            Conv2d-4          [-1, 256, 26, 26]         614,656\n",
      "              ReLU-5          [-1, 256, 26, 26]               0\n",
      "         MaxPool2d-6          [-1, 256, 12, 12]               0\n",
      "            Conv2d-7          [-1, 384, 12, 12]         885,120\n",
      "              ReLU-8          [-1, 384, 12, 12]               0\n",
      "            Conv2d-9          [-1, 384, 12, 12]       1,327,488\n",
      "             ReLU-10          [-1, 384, 12, 12]               0\n",
      "           Conv2d-11          [-1, 256, 12, 12]         884,992\n",
      "             ReLU-12          [-1, 256, 12, 12]               0\n",
      "        MaxPool2d-13            [-1, 256, 5, 5]               0\n",
      "          Flatten-14                 [-1, 6400]               0\n",
      "           Linear-15                 [-1, 4096]      26,218,496\n",
      "             ReLU-16                 [-1, 4096]               0\n",
      "          Dropout-17                 [-1, 4096]               0\n",
      "           Linear-18                 [-1, 4096]      16,781,312\n",
      "             ReLU-19                 [-1, 4096]               0\n",
      "          Dropout-20                 [-1, 4096]               0\n",
      "           Linear-21                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 46,764,746\n",
      "Trainable params: 46,764,746\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 10.22\n",
      "Params size (MB): 178.39\n",
      "Estimated Total Size (MB): 188.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net.to(device), input_size=(1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc745866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, time since epoch: 0.338. Train acc: 0.078. Train Loss: 2.302\n",
      "Step 10, time since epoch: 2.172. Train acc: 0.106. Train Loss: 2.333\n",
      "Step 20, time since epoch: 4.089. Train acc: 0.149. Train Loss: 2.281\n",
      "Step 30, time since epoch: 6.158. Train acc: 0.258. Train Loss: 2.057\n",
      "Step 40, time since epoch: 8.083. Train acc: 0.356. Train Loss: 1.799\n",
      "Step 50, time since epoch: 9.890. Train acc: 0.437. Train Loss: 1.582\n",
      "Step 60, time since epoch: 11.674. Train acc: 0.496. Train Loss: 1.419\n",
      "Step 70, time since epoch: 13.484. Train acc: 0.544. Train Loss: 1.286\n",
      "Step 80, time since epoch: 15.267. Train acc: 0.585. Train Loss: 1.175\n",
      "Step 90, time since epoch: 17.077. Train acc: 0.620. Train Loss: 1.080\n",
      "Step 100, time since epoch: 18.935. Train acc: 0.649. Train Loss: 1.001\n",
      "Step 110, time since epoch: 20.736. Train acc: 0.672. Train Loss: 0.933\n",
      "Step 120, time since epoch: 22.562. Train acc: 0.693. Train Loss: 0.877\n",
      "Step 130, time since epoch: 24.374. Train acc: 0.711. Train Loss: 0.828\n",
      "Step 140, time since epoch: 26.166. Train acc: 0.728. Train Loss: 0.782\n",
      "Step 150, time since epoch: 28.041. Train acc: 0.742. Train Loss: 0.742\n",
      "Step 160, time since epoch: 29.856. Train acc: 0.755. Train Loss: 0.706\n",
      "Step 170, time since epoch: 31.975. Train acc: 0.766. Train Loss: 0.675\n",
      "Step 180, time since epoch: 33.819. Train acc: 0.776. Train Loss: 0.649\n",
      "Step 190, time since epoch: 35.662. Train acc: 0.784. Train Loss: 0.627\n",
      "Step 200, time since epoch: 37.471. Train acc: 0.792. Train Loss: 0.604\n",
      "Step 210, time since epoch: 39.259. Train acc: 0.800. Train Loss: 0.582\n",
      "Step 220, time since epoch: 41.063. Train acc: 0.808. Train Loss: 0.560\n",
      "Step 230, time since epoch: 42.891. Train acc: 0.815. Train Loss: 0.539\n",
      "epoch 1, loss 0.5332, train acc 0.817, test acc 0.972, time 49.2 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 1\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train(net, train_iter, test_iter, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d368d",
   "metadata": {},
   "source": [
    "Хороший результат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282746bf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel01",
   "language": "python",
   "name": "kernel01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
